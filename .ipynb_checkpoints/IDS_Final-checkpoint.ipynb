{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from highlight_text import ax_text, fig_text\n",
    "\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(x):\n",
    "    return round(abs(2 * (1 / (1 + np.exp(-x)) - 0.5)), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./combined_sentiment_labelled.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(set([x for l in data.text.apply(lambda x: x.split(\" \")).values for x in l]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tutorials/text/text_classification_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "dataset = tfds.load('imdb_reviews',\n",
    "                    as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = dataset['train'], dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example, label in train_dataset.take(1):\n",
    "#   print('text: ', example.numpy())\n",
    "#   print('label: ', label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE=5000\n",
    "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(train_dataset.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "391"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum([1 for i in train_dataset.as_numpy_iterator()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"training/cp-{epoch:04d}.ckpt\"\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path, \n",
    "    verbose=1, \n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()) + 2,\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with tf.device('/GPU:0'):\n",
    "#     model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#                   optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "#                   metrics=['accuracy'])\n",
    "    \n",
    "#     model.predict(train_dataset.take(1))\n",
    "#     model.save_weights(checkpoint_path.format(epoch=0))\n",
    "    \n",
    "#     history = model.fit(train_dataset, \n",
    "#                         epochs=5,\n",
    "#                         validation_data=test_dataset, \n",
    "#                         validation_steps=30,\n",
    "#                         callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('trained_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('first_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='test accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    GREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_text(text, model=model):\n",
    "    tokens = text.split(\" \")\n",
    "    probs = [0]\n",
    "    for k in range(0,len(tokens)):\n",
    "        probs.append(model.predict(np.array([\" \".join(tokens[:k+1])]))[0][0])\n",
    "    pred = probs[-1]\n",
    "    probs = np.diff(probs)\n",
    "    colors = [bcolors.ENDC if abs(p / max(np.abs(probs))) < 0.1 \n",
    "                  else (bcolors.RED if p < 0 else bcolors.GREEN) \n",
    "              for p in probs]\n",
    "    ends = [bcolors.ENDC] * len(probs)\n",
    "    return \" \".join([c+t+e for c,t,e in zip(colors, tokens, ends)]), pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/287871/how-to-print-colored-text-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.3803856]], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([\"I had to walk out of the theatre for a few minutes just to get some relief!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | POSITIVE | 0.0 | \u001b[92mI\u001b[0m \u001b[0mhad\u001b[0m \u001b[92mto\u001b[0m \u001b[0mwalk\u001b[0m \u001b[91mout\u001b[0m \u001b[92mof\u001b[0m \u001b[91mthe\u001b[0m \u001b[91mtheatre\u001b[0m \u001b[92mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[91mfew\u001b[0m \u001b[92mminutes\u001b[0m \u001b[92mjust\u001b[0m \u001b[92mto\u001b[0m \u001b[91mget\u001b[0m \u001b[91msome\u001b[0m \u001b[92mrelief!\u001b[0m \u001b[0m\u001b[0m \u001b[0m\u001b[0m\n",
      "Epoch 1 | NEGATIVE | 0.13 | \u001b[0mI\u001b[0m \u001b[91mhad\u001b[0m \u001b[92mto\u001b[0m \u001b[91mwalk\u001b[0m \u001b[0mout\u001b[0m \u001b[92mof\u001b[0m \u001b[91mthe\u001b[0m \u001b[91mtheatre\u001b[0m \u001b[92mfor\u001b[0m \u001b[92ma\u001b[0m \u001b[91mfew\u001b[0m \u001b[91mminutes\u001b[0m \u001b[91mjust\u001b[0m \u001b[0mto\u001b[0m \u001b[91mget\u001b[0m \u001b[91msome\u001b[0m \u001b[92mrelief!\u001b[0m \u001b[0m\u001b[0m \u001b[0m\u001b[0m\n",
      "Epoch 2 | NEGATIVE | 0.4 | \u001b[92mI\u001b[0m \u001b[91mhad\u001b[0m \u001b[0mto\u001b[0m \u001b[91mwalk\u001b[0m \u001b[0mout\u001b[0m \u001b[92mof\u001b[0m \u001b[91mthe\u001b[0m \u001b[91mtheatre\u001b[0m \u001b[0mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[91mfew\u001b[0m \u001b[91mminutes\u001b[0m \u001b[91mjust\u001b[0m \u001b[91mto\u001b[0m \u001b[91mget\u001b[0m \u001b[91msome\u001b[0m \u001b[92mrelief!\u001b[0m \u001b[0m\u001b[0m \u001b[0m\u001b[0m\n",
      "Epoch 3 | NEGATIVE | 0.51 | \u001b[92mI\u001b[0m \u001b[91mhad\u001b[0m \u001b[0mto\u001b[0m \u001b[91mwalk\u001b[0m \u001b[0mout\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[91mtheatre\u001b[0m \u001b[0mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[91mfew\u001b[0m \u001b[91mminutes\u001b[0m \u001b[91mjust\u001b[0m \u001b[91mto\u001b[0m \u001b[91mget\u001b[0m \u001b[91msome\u001b[0m \u001b[0mrelief!\u001b[0m \u001b[0m\u001b[0m \u001b[0m\u001b[0m\n",
      "Epoch 4 | NEGATIVE | 0.58 | \u001b[0mI\u001b[0m \u001b[91mhad\u001b[0m \u001b[0mto\u001b[0m \u001b[91mwalk\u001b[0m \u001b[0mout\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[91mtheatre\u001b[0m \u001b[91mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfew\u001b[0m \u001b[91mminutes\u001b[0m \u001b[91mjust\u001b[0m \u001b[91mto\u001b[0m \u001b[0mget\u001b[0m \u001b[0msome\u001b[0m \u001b[0mrelief!\u001b[0m \u001b[0m\u001b[0m \u001b[0m\u001b[0m\n",
      "Epoch 5 | NEGATIVE | 0.6 | \u001b[92mI\u001b[0m \u001b[91mhad\u001b[0m \u001b[0mto\u001b[0m \u001b[91mwalk\u001b[0m \u001b[0mout\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[91mtheatre\u001b[0m \u001b[0mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfew\u001b[0m \u001b[91mminutes\u001b[0m \u001b[91mjust\u001b[0m \u001b[0mto\u001b[0m \u001b[0mget\u001b[0m \u001b[0msome\u001b[0m \u001b[0mrelief!\u001b[0m \u001b[0m\u001b[0m \u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "text = data.text[np.random.randint(0, len(data))]\n",
    "\n",
    "for i in range(6):\n",
    "    model.load_weights(f'./training/cp-000{i}.ckpt')\n",
    "    pred = color_text(text, model=model)\n",
    "    print(f\"Epoch {i}\", \"|\", \"NEGATIVE\" if pred[1] < 0 else \"POSITIVE\", \"|\", probability(pred[1]), \"|\", pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average of word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sen2vec(x):\n",
    "    return model.get_layer(name='embedding')(model.get_layer(name=\"text_vectorization\")(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sen2vec([[x] for x in data.text.values[:3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 21, 64])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# samples, words, embedding\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processed sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen2vec_model = tf.keras.Sequential([\n",
    "    model.get_layer(name=\"text_vectorization\"),\n",
    "    model.get_layer(name='embedding'),\n",
    "    model.get_layer(name='lstm'),\n",
    "    model.get_layer(name='dense')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen2vec_model_interm = tf.keras.Sequential([\n",
    "    model.get_layer(name=\"text_vectorization\"),\n",
    "    model.get_layer(name='embedding'),\n",
    "    model.get_layer(name='lstm')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data.sample(n=50).text.values\n",
    "\n",
    "tsne = TSNE()\n",
    "tsned_space_raw = tsne.fit_transform(sen2vec([[x] for x in sentences]).numpy().mean(axis=1))\n",
    "\n",
    "tsned_space_proc = tsne.fit_transform(sen2vec_model.predict(sentences))\n",
    "\n",
    "tsned_space_intermediate = tsne.fit_transform(sen2vec_model_interm.predict(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot_data = pd.DataFrame({'x_raw': tsned_space_raw[:,0], \n",
    "                               'y_raw': tsned_space_raw[:,1],\n",
    "                               'x_interm': tsned_space_intermediate[:,0], \n",
    "                               'y_interm': tsned_space_intermediate[:,1],\n",
    "                               'x_proc': tsned_space_proc[:,0], \n",
    "                               'y_proc': tsned_space_proc[:,1],\n",
    "                               'sentence': sentences, \n",
    "                               'opacity': np.abs(model.predict(sentences).reshape(-1)),\n",
    "                               'prob': model.predict(sentences).reshape(-1).round(2).astype(str),\n",
    "                               'pred': ['Positive' if x else 'Negative' \n",
    "                                        for x in (model.predict(sentences).reshape(-1) > 0)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_embs = alt.selection_interval(empty='all', encodings=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_tsned = alt.Chart(tsne_plot_data).mark_circle(size=200).encode(\n",
    "    x = 'x_raw',\n",
    "    y = 'y_raw',\n",
    "    tooltip =[alt.Tooltip('sentence'), alt.Tooltip('prob')],\n",
    "    color = alt.Color('pred', \n",
    "                      scale=alt.Scale(domain=['Negative', 'Positive'], \n",
    "                                      range=['red', 'green']), \n",
    "                      legend=None),\n",
    "    opacity=alt.condition(selector_embs, 'opacity', alt.value(0.05), legend=None)\n",
    ").properties(\n",
    "    title='Raw sentences'\n",
    ").add_selection(\n",
    "    selector_embs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "interm_tsned = alt.Chart(tsne_plot_data).mark_circle(size=200).encode(\n",
    "    x = 'x_interm',\n",
    "    y = 'y_interm',\n",
    "    tooltip =[alt.Tooltip('sentence'), alt.Tooltip('prob')],\n",
    "    color = alt.Color('pred', \n",
    "                      scale=alt.Scale(domain=['Negative', 'Positive'], \n",
    "                                      range=['red', 'green']), \n",
    "                      legend=None),\n",
    "    opacity=alt.condition(selector_embs, 'opacity', alt.value(0.05), legend=None)\n",
    ").properties(\n",
    "    title='Intermediate state sentences'\n",
    ").add_selection(\n",
    "    selector_embs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_tsned = alt.Chart(tsne_plot_data).mark_circle(size=200).encode(\n",
    "    x = 'x_proc',\n",
    "    y = 'y_proc',\n",
    "    tooltip =[alt.Tooltip('sentence'), alt.Tooltip('prob')],\n",
    "    color = alt.Color('pred', \n",
    "                      scale=alt.Scale(domain=['Negative', 'Positive'], \n",
    "                                      range=['red', 'green']), \n",
    "                      legend=None),\n",
    "    opacity=alt.condition(selector_embs, 'opacity', alt.value(0.05), legend=None)\n",
    ").properties(\n",
    "    title='Processed sentences'\n",
    ").add_selection(\n",
    "    selector_embs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-7e9e3614e5f74b83a0ca7f05b3c1148b\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-7e9e3614e5f74b83a0ca7f05b3c1148b\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-7e9e3614e5f74b83a0ca7f05b3c1148b\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"vconcat\": [{\"mark\": {\"type\": \"circle\", \"size\": 200}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"pred\", \"legend\": null, \"scale\": {\"domain\": [\"Negative\", \"Positive\"], \"range\": [\"red\", \"green\"]}}, \"opacity\": {\"condition\": {\"type\": \"quantitative\", \"field\": \"opacity\", \"legend\": null, \"selection\": \"selector001\"}, \"value\": 0.05}, \"tooltip\": [{\"type\": \"nominal\", \"field\": \"sentence\"}, {\"type\": \"nominal\", \"field\": \"prob\"}], \"x\": {\"type\": \"quantitative\", \"field\": \"x_raw\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"y_raw\"}}, \"selection\": {\"selector001\": {\"type\": \"interval\", \"empty\": \"all\", \"encodings\": [\"x\", \"y\"]}}, \"title\": \"Raw sentences\"}, {\"mark\": {\"type\": \"circle\", \"size\": 200}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"pred\", \"legend\": null, \"scale\": {\"domain\": [\"Negative\", \"Positive\"], \"range\": [\"red\", \"green\"]}}, \"opacity\": {\"condition\": {\"type\": \"quantitative\", \"field\": \"opacity\", \"legend\": null, \"selection\": \"selector001\"}, \"value\": 0.05}, \"tooltip\": [{\"type\": \"nominal\", \"field\": \"sentence\"}, {\"type\": \"nominal\", \"field\": \"prob\"}], \"x\": {\"type\": \"quantitative\", \"field\": \"x_interm\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"y_interm\"}}, \"selection\": {\"selector001\": {\"type\": \"interval\", \"empty\": \"all\", \"encodings\": [\"x\", \"y\"]}}, \"title\": \"Intermediate state sentences\"}, {\"mark\": {\"type\": \"circle\", \"size\": 200}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"pred\", \"legend\": null, \"scale\": {\"domain\": [\"Negative\", \"Positive\"], \"range\": [\"red\", \"green\"]}}, \"opacity\": {\"condition\": {\"type\": \"quantitative\", \"field\": \"opacity\", \"legend\": null, \"selection\": \"selector001\"}, \"value\": 0.05}, \"tooltip\": [{\"type\": \"nominal\", \"field\": \"sentence\"}, {\"type\": \"nominal\", \"field\": \"prob\"}], \"x\": {\"type\": \"quantitative\", \"field\": \"x_proc\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"y_proc\"}}, \"selection\": {\"selector001\": {\"type\": \"interval\", \"empty\": \"all\", \"encodings\": [\"x\", \"y\"]}}, \"title\": \"Processed sentences\"}], \"data\": {\"name\": \"data-d4165ac30100401e33852003f2dec640\"}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-d4165ac30100401e33852003f2dec640\": [{\"x_raw\": -24.231428146362305, \"y_raw\": 1.3524097204208374, \"x_interm\": 16.624475479125977, \"y_interm\": -79.4602279663086, \"x_proc\": -14.198569297790527, \"y_proc\": 2.71574068069458, \"sentence\": \"The design is very odd, as the ear \\\"clip\\\" is not very comfortable at all.\", \"opacity\": 0.5755962133407593, \"prob\": \"0.58\", \"pred\": \"Positive\"}, {\"x_raw\": -3.4836132526397705, \"y_raw\": 4.0212860107421875, \"x_interm\": -10.037778854370117, \"y_interm\": -178.39071655273438, \"x_proc\": -8.612232208251953, \"y_proc\": -21.47195816040039, \"sentence\": \"I think the most wonderful parts (literally, full of \\\"wonder\\\") are the excerpts from his works.  \", \"opacity\": 1.5271847248077393, \"prob\": \"1.53\", \"pred\": \"Positive\"}, {\"x_raw\": 2.6686346530914307, \"y_raw\": -28.31658935546875, \"x_interm\": -16.49112892150879, \"y_interm\": -71.09635162353516, \"x_proc\": 0.27353352308273315, \"y_proc\": -2.413616895675659, \"sentence\": \"Food was really good and I got full petty fast.\", \"opacity\": 0.39665111899375916, \"prob\": \"0.4\", \"pred\": \"Positive\"}, {\"x_raw\": 14.35745906829834, \"y_raw\": -25.05121612548828, \"x_interm\": 19.763519287109375, \"y_interm\": -28.079212188720703, \"x_proc\": 7.931402206420898, \"y_proc\": -1.5296101570129395, \"sentence\": \"Needless to say, I won't be going back anytime soon.\", \"opacity\": 0.23617884516716003, \"prob\": \"0.24\", \"pred\": \"Positive\"}, {\"x_raw\": 26.227924346923828, \"y_raw\": -27.960954666137695, \"x_interm\": -38.00825881958008, \"y_interm\": -122.78470611572266, \"x_proc\": -8.260940551757812, \"y_proc\": -13.815650939941406, \"sentence\": \"Punishment Park is a brilliant piece of cinema.  \", \"opacity\": 1.0234543085098267, \"prob\": \"1.02\", \"pred\": \"Positive\"}, {\"x_raw\": -15.144554138183594, \"y_raw\": 9.161352157592773, \"x_interm\": 34.092716217041016, \"y_interm\": 32.23533248901367, \"x_proc\": 2.605503797531128, \"y_proc\": 26.88776397705078, \"sentence\": \"however, my girl was complain that some time the phone doesn't wake up like normal phone does.\", \"opacity\": 0.45945093035697937, \"prob\": \"-0.46\", \"pred\": \"Negative\"}, {\"x_raw\": -5.523594856262207, \"y_raw\": 14.761970520019531, \"x_interm\": 36.10081481933594, \"y_interm\": 2.0034003257751465, \"x_proc\": 6.574520587921143, \"y_proc\": 14.895759582519531, \"sentence\": \") very bad performance plays Angela Bennett, a computer expert who is at home all the time.  \", \"opacity\": 0.28736016154289246, \"prob\": \"-0.29\", \"pred\": \"Negative\"}, {\"x_raw\": 16.088716506958008, \"y_raw\": 18.617353439331055, \"x_interm\": -17.135845184326172, \"y_interm\": -145.9384765625, \"x_proc\": -14.272542953491211, \"y_proc\": -16.852479934692383, \"sentence\": \"It is well made, easy to access the phone and has a handy, detachable belt clip.\", \"opacity\": 1.4367971420288086, \"prob\": \"1.44\", \"pred\": \"Positive\"}, {\"x_raw\": 12.909917831420898, \"y_raw\": -48.17110824584961, \"x_interm\": -17.304162979125977, \"y_interm\": -101.94440460205078, \"x_proc\": -11.97813892364502, \"y_proc\": -8.758618354797363, \"sentence\": \"Great Pocket PC / phone combination.\", \"opacity\": 0.8407449126243591, \"prob\": \"0.84\", \"pred\": \"Positive\"}, {\"x_raw\": -13.07811450958252, \"y_raw\": -2.89774751663208, \"x_interm\": -29.02435302734375, \"y_interm\": -47.65412139892578, \"x_proc\": -1.8127450942993164, \"y_proc\": 3.8500683307647705, \"sentence\": \"I exchanged the sony ericson z500a for this and I'm pretty happy with that decision.\", \"opacity\": 0.34734848141670227, \"prob\": \"0.35\", \"pred\": \"Positive\"}, {\"x_raw\": 33.737770080566406, \"y_raw\": -40.46290969848633, \"x_interm\": 11.261079788208008, \"y_interm\": 58.182159423828125, \"x_proc\": 16.907604217529297, \"y_proc\": 20.35137367248535, \"sentence\": \"much better than the hard plastic cases.\", \"opacity\": 0.7234346270561218, \"prob\": \"-0.72\", \"pred\": \"Negative\"}, {\"x_raw\": 5.691613674163818, \"y_raw\": -69.69603729248047, \"x_interm\": 59.63606643676758, \"y_interm\": 181.66456604003906, \"x_proc\": 26.2196102142334, \"y_proc\": 43.99896240234375, \"sentence\": \"Utter crap.. Sound quality is TERRIBLE.\", \"opacity\": 2.0019915103912354, \"prob\": \"-2.0\", \"pred\": \"Negative\"}, {\"x_raw\": -31.357616424560547, \"y_raw\": 66.62846374511719, \"x_interm\": -23.262454986572266, \"y_interm\": -8.4341459274292, \"x_proc\": 0.3990805447101593, \"y_proc\": 15.18491268157959, \"sentence\": \"If you are looking for a movie with a terrific cast, some good music(including a Shirley Jones rendition of \\\"The Way You Look Tonight\\\"), and an uplifting ending, give this one a try.  \", \"opacity\": 0.14056767523288727, \"prob\": \"-0.14\", \"pred\": \"Negative\"}, {\"x_raw\": -16.39704132080078, \"y_raw\": 34.10044860839844, \"x_interm\": 70.05524444580078, \"y_interm\": 123.94610595703125, \"x_proc\": 18.708494186401367, \"y_proc\": 44.64887619018555, \"sentence\": \"All this movie does is make you sick watching all these slackers make excuses for their stupid actions for 90 minutes.  \", \"opacity\": 1.774921178817749, \"prob\": \"-1.77\", \"pred\": \"Negative\"}, {\"x_raw\": 18.564842224121094, \"y_raw\": -73.4972915649414, \"x_interm\": -10.721803665161133, \"y_interm\": 108.14469146728516, \"x_proc\": 21.312707901000977, \"y_proc\": 26.129262924194336, \"sentence\": \"Bad Reception.\", \"opacity\": 0.9361379742622375, \"prob\": \"-0.94\", \"pred\": \"Negative\"}, {\"x_raw\": -28.67985725402832, \"y_raw\": 53.27881622314453, \"x_interm\": 14.392840385437012, \"y_interm\": -153.92572021484375, \"x_proc\": -15.665643692016602, \"y_proc\": -23.82661247253418, \"sentence\": \"The only consistent thread holding the series together were the amazing performances of Leni Parker and Anita LaSelva as the two Taelons in quiet idealogical conflict.  \", \"opacity\": 1.815281867980957, \"prob\": \"1.82\", \"pred\": \"Positive\"}, {\"x_raw\": 21.644357681274414, \"y_raw\": -40.682559967041016, \"x_interm\": -66.85752868652344, \"y_interm\": -108.11214447021484, \"x_proc\": -1.754486083984375, \"y_proc\": -13.527887344360352, \"sentence\": \"It is worth the drive.\", \"opacity\": 0.8539204001426697, \"prob\": \"0.85\", \"pred\": \"Positive\"}, {\"x_raw\": -7.5772809982299805, \"y_raw\": 73.01314544677734, \"x_interm\": 105.63842010498047, \"y_interm\": 110.08998107910156, \"x_proc\": 30.80372428894043, \"y_proc\": 50.400604248046875, \"sentence\": \"A cheap and cheerless heist movie with poor characterisation, lots of underbite style stoic emoting (think Chow Yun Fat in A Better Tomorrow) and some cheesy clich\\u00e9s thrown into an abandoned factory ready for a few poorly executed flying judo rolls a la John Woo.  \", \"opacity\": 3.070566177368164, \"prob\": \"-3.07\", \"pred\": \"Negative\"}, {\"x_raw\": -30.045866012573242, \"y_raw\": 13.60706901550293, \"x_interm\": -62.50571823120117, \"y_interm\": -141.51730346679688, \"x_proc\": -18.51768684387207, \"y_proc\": -10.957212448120117, \"sentence\": \"This case has passed the one year mark and while it shows signs of wear, it is 100% functional.\", \"opacity\": 1.1674246788024902, \"prob\": \"1.17\", \"pred\": \"Positive\"}, {\"x_raw\": -8.692784309387207, \"y_raw\": -50.639434814453125, \"x_interm\": -14.060347557067871, \"y_interm\": 75.55863189697266, \"x_proc\": 9.674308776855469, \"y_proc\": 25.62758445739746, \"sentence\": \"There was hardly any meat.\", \"opacity\": 0.7617791891098022, \"prob\": \"-0.76\", \"pred\": \"Negative\"}, {\"x_raw\": -16.126333236694336, \"y_raw\": 48.02956771850586, \"x_interm\": -49.02048873901367, \"y_interm\": 59.71867752075195, \"x_proc\": 5.137418270111084, \"y_proc\": 20.82124137878418, \"sentence\": \"I was proven dead wrong by this sushi bar, not only because the quality is great, but the service is fast and the food, impeccable.\", \"opacity\": 0.436278760433197, \"prob\": \"-0.44\", \"pred\": \"Negative\"}, {\"x_raw\": 1.9577473402023315, \"y_raw\": 49.24028396606445, \"x_interm\": 100.85008239746094, \"y_interm\": 142.6595001220703, \"x_proc\": 23.6376953125, \"y_proc\": 50.242919921875, \"sentence\": \"IMDB ratings only go as low 1 for awful, it's time to get some negative numbers in there for cases such as these.  \", \"opacity\": 2.374866485595703, \"prob\": \"-2.37\", \"pred\": \"Negative\"}, {\"x_raw\": -20.340290069580078, \"y_raw\": 75.7458724975586, \"x_interm\": -75.23162841796875, \"y_interm\": -46.013919830322266, \"x_proc\": -5.38633394241333, \"y_proc\": -0.7572755813598633, \"sentence\": \"The film has great actors, a master director, a significant theme--at least a would-be significant theme, undertone of fifties existential world-weariness, aerial scenes that ought to have thrilled both senses and imagination, and characters about which one might deeply care.  \", \"opacity\": 0.3696371018886566, \"prob\": \"0.37\", \"pred\": \"Positive\"}, {\"x_raw\": -2.903045415878296, \"y_raw\": -7.182842254638672, \"x_interm\": -50.40814208984375, \"y_interm\": -64.84294891357422, \"x_proc\": 2.3951799869537354, \"y_proc\": -8.06286334991455, \"sentence\": \"I love this cable - it allows me to connect any mini-USB device to my PC.\", \"opacity\": 0.5943487286567688, \"prob\": \"0.59\", \"pred\": \"Positive\"}, {\"x_raw\": -5.968246936798096, \"y_raw\": -18.342363357543945, \"x_interm\": 42.42012405395508, \"y_interm\": 63.42851257324219, \"x_proc\": 8.821118354797363, \"y_proc\": 31.64156150817871, \"sentence\": \"It is indescribably the most annoying and idiotic show I have ever seen.  \", \"opacity\": 0.7622388005256653, \"prob\": \"-0.76\", \"pred\": \"Negative\"}, {\"x_raw\": 15.56109619140625, \"y_raw\": -60.61278533935547, \"x_interm\": -17.77229118347168, \"y_interm\": 47.254791259765625, \"x_proc\": 18.77080535888672, \"y_proc\": 13.6044921875, \"sentence\": \"Does not fit.\", \"opacity\": 0.48964136838912964, \"prob\": \"-0.49\", \"pred\": \"Negative\"}, {\"x_raw\": 24.564851760864258, \"y_raw\": -52.4283332824707, \"x_interm\": 13.233741760253906, \"y_interm\": 85.47102355957031, \"x_proc\": 15.15542221069336, \"y_proc\": 26.290176391601562, \"sentence\": \"How stupid is that?\", \"opacity\": 0.7832523584365845, \"prob\": \"-0.78\", \"pred\": \"Negative\"}, {\"x_raw\": 6.572805404663086, \"y_raw\": 9.135149955749512, \"x_interm\": -71.6220703125, \"y_interm\": 22.681934356689453, \"x_proc\": 2.971451759338379, \"y_proc\": 9.78923511505127, \"sentence\": \"The characters were all funny and had the peculiarity of not having a true lead character.  \", \"opacity\": 0.12313863635063171, \"prob\": \"-0.12\", \"pred\": \"Negative\"}, {\"x_raw\": 37.653045654296875, \"y_raw\": -54.301795959472656, \"x_interm\": 69.85652160644531, \"y_interm\": 155.06137084960938, \"x_proc\": 32.30204391479492, \"y_proc\": 41.64954376220703, \"sentence\": \"The WORST EXPERIENCE EVER.\", \"opacity\": 1.8525415658950806, \"prob\": \"-1.85\", \"pred\": \"Negative\"}, {\"x_raw\": 7.21411657333374, \"y_raw\": -2.4749932289123535, \"x_interm\": -45.09430694580078, \"y_interm\": 4.854803562164307, \"x_proc\": -1.5566294193267822, \"y_proc\": 21.457378387451172, \"sentence\": \"I love the Pho and the spring rolls oh so yummy you have to try.\", \"opacity\": 0.3102337121963501, \"prob\": \"-0.31\", \"pred\": \"Negative\"}, {\"x_raw\": 28.31557273864746, \"y_raw\": -64.20714569091797, \"x_interm\": 10.481385231018066, \"y_interm\": -107.91510772705078, \"x_proc\": -10.83340835571289, \"y_proc\": -3.1528873443603516, \"sentence\": \"This is a beautiful phone.\", \"opacity\": 0.7915299534797668, \"prob\": \"0.79\", \"pred\": \"Positive\"}, {\"x_raw\": -38.12811279296875, \"y_raw\": 31.190393447875977, \"x_interm\": -47.17167663574219, \"y_interm\": -28.830904006958008, \"x_proc\": -3.913316011428833, \"y_proc\": 9.345693588256836, \"sentence\": \"My brother in law who works at the mall ate here same day, and guess what he was sick all night too.\", \"opacity\": 0.21679700911045074, \"prob\": \"0.22\", \"pred\": \"Positive\"}, {\"x_raw\": -17.910858154296875, \"y_raw\": -14.407561302185059, \"x_interm\": 4.248284339904785, \"y_interm\": 7.064081728458405e-05, \"x_proc\": 8.856732368469238, \"y_proc\": 8.382668495178223, \"sentence\": \"I also didn't like the \\\"on\\\" button, it felt like it would crack with use.\", \"opacity\": 0.22414781153202057, \"prob\": \"-0.22\", \"pred\": \"Negative\"}, {\"x_raw\": -6.909375190734863, \"y_raw\": 27.200424194335938, \"x_interm\": -71.02845764160156, \"y_interm\": -10.404807090759277, \"x_proc\": -6.1691813468933105, \"y_proc\": 15.592429161071777, \"sentence\": \"I really like this product over the Motorola because it is allot clearer on the ear piece and the mic.\", \"opacity\": 0.023388005793094635, \"prob\": \"-0.02\", \"pred\": \"Negative\"}, {\"x_raw\": -40.80998229980469, \"y_raw\": 51.99899673461914, \"x_interm\": -42.8250846862793, \"y_interm\": -90.5525894165039, \"x_proc\": -17.473876953125, \"y_proc\": -3.371896982192993, \"sentence\": \"Lovely little thriller from Hitchcock, with lots of nice shenanigans surrounding a murdered spy, a kidnapped child, a nasty church, a foreign plot and some random taxidermists.  \", \"opacity\": 0.7724134922027588, \"prob\": \"0.77\", \"pred\": \"Positive\"}, {\"x_raw\": -17.24968910217285, \"y_raw\": 60.92621994018555, \"x_interm\": 6.997710704803467, \"y_interm\": 29.162635803222656, \"x_proc\": 10.9797945022583, \"y_proc\": 19.329879760742188, \"sentence\": \"The best example of how dumb the writing is when it's established that you can turn the zombie-students back into humans by removing a necklace containing a piece of the meteorite.  \", \"opacity\": 0.44896355271339417, \"prob\": \"-0.45\", \"pred\": \"Negative\"}, {\"x_raw\": -28.1915283203125, \"y_raw\": 41.337059020996094, \"x_interm\": -9.191534996032715, \"y_interm\": -30.98272705078125, \"x_proc\": 3.7602391242980957, \"y_proc\": 2.916581153869629, \"sentence\": \"The handsfree part works fine, but then the car tries to download the address book, and the Treo reboots.Overall, I still rate this device high.\", \"opacity\": 0.279739648103714, \"prob\": \"0.28\", \"pred\": \"Positive\"}, {\"x_raw\": -17.571792602539062, \"y_raw\": 20.71162223815918, \"x_interm\": -38.783241271972656, \"y_interm\": 91.35012817382812, \"x_proc\": 23.48453140258789, \"y_proc\": 19.830902099609375, \"sentence\": \"Lobster Bisque, Bussell Sprouts, Risotto, Filet ALL needed salt and pepper..and of course there is none at the tables.\", \"opacity\": 0.8015876412391663, \"prob\": \"-0.8\", \"pred\": \"Negative\"}, {\"x_raw\": 7.417226314544678, \"y_raw\": -15.207748413085938, \"x_interm\": -78.06401062011719, \"y_interm\": -81.24073028564453, \"x_proc\": -5.383205890655518, \"y_proc\": -7.5766448974609375, \"sentence\": \"I love Lane, but I've never seen her in a movie this lousy.  \", \"opacity\": 0.8077207207679749, \"prob\": \"0.81\", \"pred\": \"Positive\"}, {\"x_raw\": -26.507368087768555, \"y_raw\": 27.813217163085938, \"x_interm\": 44.68486785888672, \"y_interm\": 138.68711853027344, \"x_proc\": 22.08588981628418, \"y_proc\": 39.125144958496094, \"sentence\": \"No shifting, no bubbling, no peeling, not even a scratch, NOTHING!I couldn't be more happier with my new one for the Droid.\", \"opacity\": 1.6034018993377686, \"prob\": \"-1.6\", \"pred\": \"Negative\"}, {\"x_raw\": 5.529362201690674, \"y_raw\": 34.60563659667969, \"x_interm\": 38.847103118896484, \"y_interm\": -53.0667839050293, \"x_proc\": -7.886409759521484, \"y_proc\": 4.6746625900268555, \"sentence\": \"I kept catching the cable on the seat and I had to pull the phone out to turn it on an off.\", \"opacity\": 0.2944486439228058, \"prob\": \"0.29\", \"pred\": \"Positive\"}, {\"x_raw\": 4.084990978240967, \"y_raw\": 21.042930603027344, \"x_interm\": 30.942447662353516, \"y_interm\": 166.08665466308594, \"x_proc\": 28.61946678161621, \"y_proc\": 36.49799346923828, \"sentence\": \"My husband said she was very rude... did not even apologize for the bad food or anything.\", \"opacity\": 1.6148219108581543, \"prob\": \"-1.61\", \"pred\": \"Negative\"}, {\"x_raw\": 10.642669677734375, \"y_raw\": -35.772056579589844, \"x_interm\": -42.75863265991211, \"y_interm\": 31.879409790039062, \"x_proc\": 15.059285163879395, \"y_proc\": 7.065145969390869, \"sentence\": \"Nice quality build, unlike some cheap s*** out there.\", \"opacity\": 0.29696938395500183, \"prob\": \"-0.3\", \"pred\": \"Negative\"}, {\"x_raw\": -6.215443134307861, \"y_raw\": 39.9560661315918, \"x_interm\": -19.219999313354492, \"y_interm\": 144.43617248535156, \"x_proc\": 28.487640380859375, \"y_proc\": 28.956796646118164, \"sentence\": \"I know that Jim O'Connor was very energetic and that nobody could be as much as him, but George was well dull.  \", \"opacity\": 1.2790275812149048, \"prob\": \"-1.28\", \"pred\": \"Negative\"}, {\"x_raw\": 20.692710876464844, \"y_raw\": -12.236815452575684, \"x_interm\": -41.43098068237305, \"y_interm\": -169.5281524658203, \"x_proc\": -20.86566162109375, \"y_proc\": -18.511547088623047, \"sentence\": \"Great food and great service in a clean and friendly setting.\", \"opacity\": 1.6588096618652344, \"prob\": \"1.66\", \"pred\": \"Positive\"}, {\"x_raw\": -5.888967990875244, \"y_raw\": -63.63926696777344, \"x_interm\": 4.2221999168396, \"y_interm\": -53.664066314697266, \"x_proc\": -11.792067527770996, \"y_proc\": 9.644960403442383, \"sentence\": \"Love This Phone.\", \"opacity\": 0.363887220621109, \"prob\": \"0.36\", \"pred\": \"Positive\"}, {\"x_raw\": 4.514797210693359, \"y_raw\": -56.154571533203125, \"x_interm\": -17.510414123535156, \"y_interm\": 19.591346740722656, \"x_proc\": 12.444978713989258, \"y_proc\": 13.523098945617676, \"sentence\": \"The dialogue is atrocious.  \", \"opacity\": 0.34217512607574463, \"prob\": \"-0.34\", \"pred\": \"Negative\"}, {\"x_raw\": 18.53647804260254, \"y_raw\": 4.620478630065918, \"x_interm\": 10.616972923278809, \"y_interm\": 142.9298095703125, \"x_proc\": 23.178878784179688, \"y_proc\": 32.81776428222656, \"sentence\": \"This is a VERY average phone with bad battery life that operates on a weak network.\", \"opacity\": 1.2875263690948486, \"prob\": \"-1.29\", \"pred\": \"Negative\"}, {\"x_raw\": -8.726475715637207, \"y_raw\": -32.8941764831543, \"x_interm\": 20.089942932128906, \"y_interm\": 116.9197998046875, \"x_proc\": 17.189516067504883, \"y_proc\": 32.63994598388672, \"sentence\": \"Unfortunately, it only set us up for disapppointment with our entrees.\", \"opacity\": 1.1649028062820435, \"prob\": \"-1.16\", \"pred\": \"Negative\"}, {\"x_raw\": 1.3027313947677612, \"y_raw\": -43.849693298339844, \"x_interm\": 44.26929473876953, \"y_interm\": 102.68802642822266, \"x_proc\": 14.200986862182617, \"y_proc\": 38.00621032714844, \"sentence\": \"This place is horrible and way overpriced.\", \"opacity\": 1.223589539527893, \"prob\": \"-1.22\", \"pred\": \"Negative\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.VConcatChart(...)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_tsned & interm_tsned & sentences_tsned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
